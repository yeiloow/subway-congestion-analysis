{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6a8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CSV or XLSX files found in 서울교통공사_지하철혼잡도정보_20241231.csv\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Configuration\n",
    "DB_PATH = \"db/subway.db\"\n",
    "GIS_PATH = \"data/01_raw/06_map/AL_D010_11_20260104/AL_D010_11_20260104.shp\"\n",
    "OUTPUT_PATH = \"output/station_catchment_stats.csv\"\n",
    "\n",
    "# Columns in Shapefile (Inferred from inspection)\n",
    "# A9: Usage (e.g., '단독주택', '공동주택')\n",
    "# A18: Area (N)\n",
    "# A26: Households (N) - Sum this\n",
    "# A27: Families (N) - Maybe sum this too? Let's check logic. Usually households is mostly relevant.\n",
    "# We will sum A26 + A27 for \"Total Households\" just in case, or keep them separate.\n",
    "# Based on common sense, A26 (Households) is likely the primary metric for residential units.\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"1. Loading Stations from DB...\")\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        sr.station_id,\n",
    "        s.station_name_kr as station_name,\n",
    "        l.line_name,\n",
    "        sr.lat,\n",
    "        sr.lon\n",
    "    FROM Station_Routes sr\n",
    "    JOIN Stations s ON sr.station_id = s.station_id\n",
    "    JOIN Lines l ON sr.line_id = l.line_id\n",
    "    WHERE sr.lat IS NOT NULL AND sr.lon IS NOT NULL\n",
    "    \"\"\"\n",
    "    stations_df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"   Loaded {len(stations_df)} stations.\")\n",
    "\n",
    "    # Convert to GeoDataFrame (WGS84)\n",
    "    geometry = [Point(xy) for xy in zip(stations_df.lon, stations_df.lat)]\n",
    "    stations_gdf = gpd.GeoDataFrame(stations_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "    # Project to EPSG:5186 (Korea Central Belt 2010 - Matches GIS Data)\n",
    "    print(\"2. Projecting Stations to EPSG:5186...\")\n",
    "    stations_gdf = stations_gdf.to_crs(\"EPSG:5186\")\n",
    "\n",
    "    # Create 500m Buffers\n",
    "    print(\"3. Creating 500m Buffers...\")\n",
    "    stations_gdf[\"geometry\"] = stations_gdf.geometry.buffer(500)\n",
    "\n",
    "    # Load GIS Data\n",
    "    print(\"4. Loading Building GIS Data (This may take a moment)...\")\n",
    "    # Columns:\n",
    "    # A9: Usage, A11: Structure, A13: Approval Date, A16: Height, A18: Area,\n",
    "    # A24: Building Name, A25: Detail Name, A26: Households, A27: Families\n",
    "    # Note: 'include_fields' in read_file might not work for all drivers/versions perfectly,\n",
    "    # but we will try to restrict if possible or just filter after.\n",
    "    # geopandas read_file supports 'ignore_fields' or 'include_fields' with Pyogrio engine,\n",
    "    # but standard fiona based might load all. We'll filter after loading to be safe and compatible.\n",
    "\n",
    "    buildings_gdf = gpd.read_file(GIS_PATH, encoding=\"cp949\")\n",
    "\n",
    "    # Filter columns\n",
    "    target_cols = [\n",
    "        \"A9\",\n",
    "        \"A11\",\n",
    "        \"A13\",\n",
    "        \"A16\",\n",
    "        \"A18\",\n",
    "        \"A24\",\n",
    "        \"A25\",\n",
    "        \"A26\",\n",
    "        \"A27\",\n",
    "        \"geometry\",\n",
    "    ]\n",
    "    # Check if columns exist\n",
    "    missing_cols = [c for c in target_cols if c not in buildings_gdf.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing columns in GIS data: {missing_cols}\")\n",
    "\n",
    "    # Keep only available target columns\n",
    "    available_cols = [c for c in target_cols if c in buildings_gdf.columns]\n",
    "    buildings_gdf = buildings_gdf[available_cols]\n",
    "\n",
    "    # Ensure CRS matches\n",
    "    if buildings_gdf.crs is None:\n",
    "        print(\"   Buildings CRS is missing. Setting to EPSG:5186.\")\n",
    "        buildings_gdf.set_crs(\"EPSG:5186\", inplace=True)\n",
    "    elif buildings_gdf.crs != \"EPSG:5186\":\n",
    "        print(f\"   Buildings CRS is {buildings_gdf.crs}. Reprojecting to EPSG:5186.\")\n",
    "        buildings_gdf = buildings_gdf.to_crs(\"EPSG:5186\")\n",
    "\n",
    "    # Spatial Join\n",
    "    print(\"5. Performing Spatial Join (Stations <-> Buildings)...\")\n",
    "    joined_gdf = gpd.sjoin(\n",
    "        buildings_gdf, stations_gdf, how=\"inner\", predicate=\"intersects\"\n",
    "    )\n",
    "\n",
    "    print(f\"   Matches found: {len(joined_gdf)}\")\n",
    "\n",
    "    # Prepare Data for DB Insertion\n",
    "    print(\"6. Preparing data for database insertion...\")\n",
    "\n",
    "    # Rename for DB\n",
    "    db_data = joined_gdf.rename(\n",
    "        columns={\n",
    "            \"station_id\": \"station_id\",\n",
    "            \"A24\": \"building_name\",\n",
    "            \"A25\": \"building_detail_name\",\n",
    "            \"A9\": \"usage_type\",\n",
    "            \"A11\": \"structure_type\",\n",
    "            \"A13\": \"approval_date\",\n",
    "            \"A16\": \"height\",\n",
    "            \"A18\": \"floor_area\",\n",
    "            \"A26\": \"households\",\n",
    "            \"A27\": \"families\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Select columns matching the table schema\n",
    "    db_cols = [\n",
    "        \"station_id\",\n",
    "        \"building_name\",\n",
    "        \"building_detail_name\",\n",
    "        \"usage_type\",\n",
    "        \"structure_type\",\n",
    "        \"approval_date\",\n",
    "        \"height\",\n",
    "        \"floor_area\",\n",
    "        \"households\",\n",
    "        \"families\",\n",
    "    ]\n",
    "\n",
    "    # Fill NaNs with appropriate values (None for objects, 0 for numeric if needed, but SQL handles NULL)\n",
    "    # Actually pandas NaN -> SQL NULL is automatic with to_sql usually,\n",
    "    # but let's be explicit if needed.\n",
    "\n",
    "    insert_df = db_data[db_cols].copy()\n",
    "\n",
    "    # Create Table if not exists (Double check)\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Station_Catchment_Buildings (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        station_id INTEGER NOT NULL,\n",
    "        building_name TEXT,       -- A24\n",
    "        building_detail_name TEXT,-- A25\n",
    "        usage_type TEXT,          -- A9\n",
    "        structure_type TEXT,      -- A11\n",
    "        approval_date TEXT,       -- A13\n",
    "        height REAL,              -- A16\n",
    "        floor_area REAL,          -- A18 (연면적)\n",
    "        households INTEGER,       -- A26 (세대수)\n",
    "        families INTEGER,         -- A27 (가구수)\n",
    "        FOREIGN KEY (station_id) REFERENCES Stations(station_id) ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "    # Clear existing data?\n",
    "    # Maybe we should clear data to avoid duplicates if re-run.\n",
    "    print(\"   Clearing existing catchment building data...\")\n",
    "    cursor.execute(\"DELETE FROM Station_Catchment_Buildings\")\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"   Inserting {len(insert_df)} rows into Station_Catchment_Buildings...\")\n",
    "    insert_df.to_sql(\n",
    "        \"Station_Catchment_Buildings\", conn, if_exists=\"append\", index=False\n",
    "    )\n",
    "    conn.close()\n",
    "\n",
    "    # Aggregate for CSV (Legacy support / Summary)\n",
    "    print(\"7. Aggregating Statistics for Summary CSV...\")\n",
    "    stats = (\n",
    "        joined_gdf.groupby([\"station_id\", \"station_name\", \"line_name\", \"A9\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"A18\": \"sum\",\n",
    "                \"A26\": \"sum\",\n",
    "                \"A27\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    stats.rename(\n",
    "        columns={\n",
    "            \"A9\": \"usage_type\",\n",
    "            \"A18\": \"total_area\",\n",
    "            \"A26\": \"total_households\",\n",
    "            \"A27\": \"total_families\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Save Results\n",
    "    print(f\"8. Saving summary to {OUTPUT_PATH}...\")\n",
    "    stats.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
